{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport warnings\n\nfrom glob import glob\nfrom IPython.display import display\nfrom pathlib import Path\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom typing import Any\n\nwarnings.filterwarnings('ignore')\n\nROOT      = Path('/kaggle/input/home-credit-credit-risk-model-stability')\nTRAIN_DIR = ROOT / 'parquet_files' / 'train'\nTEST_DIR  = ROOT / 'parquet_files' / 'test'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-09T06:10:11.315554Z","iopub.execute_input":"2024-05-09T06:10:11.315943Z","iopub.status.idle":"2024-05-09T06:10:17.943045Z","shell.execute_reply.started":"2024-05-09T06:10:11.315912Z","shell.execute_reply":"2024-05-09T06:10:17.942039Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class DTypeHandler():\n    @staticmethod\n    def get_feat_defs(ending_with: str):\n        feat_defs: pl.DataFrame = pl.read_csv(ROOT / 'feature_definitions.csv')\n\n        filtered_feats: pl.DataFrame = feat_defs.filter(pl.col('Variable').apply(lambda var: var.endswith(ending_with)))\n\n        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n            print(filtered_feats)\n\n        filtered_feats = None\n        feat_defs = None\n\n     \n    @staticmethod\n    def find_index(lst: list, item: Any) -> int | None:\n        try:\n            return lst.index(item)\n        except ValueError:\n            return None\n\n    \n    @staticmethod\n    def dtype_to_str(dtype: pl.DataType) -> str:\n        dtype_map = {\n            pl.Decimal: 'Decimal',\n\n            pl.Float32: 'Float32',\n            pl.Float64: 'Float64',\n\n            pl.UInt8: 'UInt8',\n            pl.UInt16: 'UInt16',\n            pl.UInt32: 'UInt32',\n            pl.UInt64: 'UInt64',\n\n            pl.Int8: 'Int8',\n            pl.Int16: 'Int16',\n            pl.Int32: 'Int32',\n            pl.Int64: 'Int64',\n\n            pl.Date: 'Date',\n            pl.Datetime: 'Datetime',\n            pl.Duration: 'Duration',\n            pl.Time: 'Time',\n\n            pl.Array: 'Array',\n            pl.List: 'List',\n            pl.Struct: 'Struct',\n\n            pl.String: 'String',\n            pl.Categorical: 'Categorical',\n            pl.Enum: 'Enum',\n            pl.Utf8: 'Utf8',\n\n            pl.Binary: 'Binary',\n            pl.Boolean: 'Boolean',\n            pl.Null: 'Null',\n            pl.Object: 'Object',\n            pl.Unknown: 'Unknown'\n        }\n\n        return dtype_map.get(dtype)\n\n    \n    @staticmethod\n    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n        feat_defs: pl.DataFrame = pl.read_csv(ROOT / 'feature_definitions.csv').filter(pl.col('Variable').apply(lambda var: var.endswith(ending_with)))\n        feat_defs.sort(by=['Variable'])\n\n        feats: list = feat_defs['Variable'].to_list()\n        feats.sort()\n\n        occurrences: list = [[set(), set()] for _ in range(feat_defs.height)]\n\n        for path in glob(str(regex_path)):\n            df_schema: dict = pl.read_parquet_schema(path)\n\n            for (feat, dtype) in df_schema.items():\n                index: int = DTypeHandler.find_index(feats, feat)\n                if index != None:\n                    occurrences[index][0].add(DTypeHandler.dtype_to_str(dtype))\n                    occurrences[index][1].add(Path(path).stem)\n\n        data_types: list[str] = [None] * feat_defs.height\n        file_locs: list[str] = [None] * feat_defs.height\n\n        for i, feat in enumerate(feats):\n            data_types[i] = list(occurrences[i][0])\n            file_locs[i] = list(occurrences[i][1])\n\n        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias('Data_Type(s)'))\n        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias('File_Loc(s)'))\n\n        return feat_defs\n\n    @staticmethod\n    def change_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n        for col in df.columns:\n            if col in ['case_id', 'WEEK_NUM', 'num_group1', 'num_group2']:\n                df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n            elif col == 'date_decision':\n                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n            # Predictors belonging to 'P - Transform DPD (Days past due)' and 'A - Transform amount' must be floats.\n            elif col[-1] in ['P', 'A']:\n                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n            # Predictors belonging to 'D - Transform date' are dates.\n            elif col[-1] == 'D':\n                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n            elif col[-1] in ('M',):\n                    df = df.with_columns(pl.col(col).cast(pl.String));\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:10:17.945452Z","iopub.execute_input":"2024-05-09T06:10:17.945863Z","iopub.status.idle":"2024-05-09T06:10:17.972134Z","shell.execute_reply.started":"2024-05-09T06:10:17.945827Z","shell.execute_reply":"2024-05-09T06:10:17.971023Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# feat_defs: pl.DataFrame = DTypeHandler.find_feat_occur(TRAIN_DIR / 'train_*.parquet', 'P')\n# feat_defs: pl.DataFrame = DTypeHandler.find_feat_occur(TRAIN_DIR / 'train_*.parquet', 'M')\n# feat_defs: pl.DataFrame = DTypeHandler.find_feat_occur(TRAIN_DIR / 'train_*.parquet', 'A')\n# feat_defs: pl.DataFrame = DTypeHandler.find_feat_occur(TRAIN_DIR / 'train_*.parquet', 'D')\nfeat_defs: pl.DataFrame = DTypeHandler.find_feat_occur(TRAIN_DIR / 'train_*.parquet', 'T')\n# feat_defs: pl.DataFrame = DTypeHandler.find_feat_occur(TRAIN_DIR / 'train_*.parquet', 'L')\n# feat_defs: pl.DataFrame = pl.read_csv(ROOT / 'feature_definitions.csv')\nwith pl.Config(fmt_str_lengths=1000, tbl_rows=-1, tbl_width_chars=180):\n    print(feat_defs)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:10:17.973619Z","iopub.execute_input":"2024-05-09T06:10:17.974003Z","iopub.status.idle":"2024-05-09T06:10:18.454395Z","shell.execute_reply.started":"2024-05-09T06:10:17.973969Z","shell.execute_reply":"2024-05-09T06:10:18.453178Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"shape: (22, 4)\n┌────────────────────────────────┬─────────────────────────────────────────────────────────────────┬──────────────┬────────────────────────────────────────────────────────────────┐\n│ Variable                       ┆ Description                                                     ┆ Data_Type(s) ┆ File_Loc(s)                                                    │\n│ ---                            ┆ ---                                                             ┆ ---          ┆ ---                                                            │\n│ str                            ┆ str                                                             ┆ list[str]    ┆ list[str]                                                      │\n╞════════════════════════════════╪═════════════════════════════════════════════════════════════════╪══════════════╪════════════════════════════════════════════════════════════════╡\n│ dpdmaxdatemonth_442T           ┆ Max DPD occurrence month for terminated contracts from credit   ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆ bureau data.                                                    ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ dpdmaxdatemonth_804T           ┆ Month when the maximum Day Past Due (DPD) occurred for active   ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_b_1\"]                                    │\n│                                ┆ contracts on credit bureau's records.                           ┆              ┆                                                                │\n│ dpdmaxdatemonth_89T            ┆ Month when maximum days past due occurred on the active         ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆ contract with the credit bureau.                                ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ dpdmaxdateyear_596T            ┆ Year when maximum Days Past Due (DPD) occurred for the active   ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆ contract.                                                       ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ dpdmaxdateyear_742T            ┆ Year of the maximum Days Past Due (DPD) on an active credit     ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_b_1\"]                                    │\n│                                ┆ contract in the credit bureau.                                  ┆              ┆                                                                │\n│ dpdmaxdateyear_896T            ┆ Year of maximum Days Past Due of closed contract obtained from  ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆ Credit Bureau.                                                  ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ incometype_1044T               ┆ Type of income of the person                                    ┆ [\"Utf8\"]     ┆ [\"train_person_1\"]                                             │\n│ overdueamountmaxdatemonth_284T ┆ Month when the maximum past due amount occurred for a closed    ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆ contract.                                                       ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ overdueamountmaxdatemonth_365T ┆ Month when maximum past due amount occurred for an active       ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆ contract.                                                       ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ overdueamountmaxdatemonth_494T ┆ Month when the maximum past due amount was recorded for an      ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_b_1\"]                                    │\n│                                ┆ active contract with the credit bureau.                         ┆              ┆                                                                │\n│ overdueamountmaxdateyear_2T    ┆ Year when the maximum past due amount occurred for active       ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆ contracts.                                                      ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ overdueamountmaxdateyear_432T  ┆ Year when max past due amount occurred for active contract.     ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_b_1\"]                                    │\n│ overdueamountmaxdateyear_994T  ┆ Year when maximum past due amount occurred for closed contract. ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_1_2\", \"train_credit_bureau_a_1_3\", …   │\n│                                ┆                                                                 ┆              ┆ \"train_credit_bureau_a_1_1\"]                                   │\n│ pmts_month_158T                ┆ Month of payment for a closed contract (num_group1 - existing   ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_2_4\", \"train_credit_bureau_a_2_9\", …   │\n│                                ┆ contract, num_group2 - payment).                                ┆              ┆ \"train_credit_bureau_a_2_7\"]                                   │\n│ pmts_month_706T                ┆ Month of payment for active contract (num_group1 - terminated   ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_2_4\", \"train_credit_bureau_a_2_9\", …   │\n│                                ┆ contract, num_group2 - payment).                                ┆              ┆ \"train_credit_bureau_a_2_7\"]                                   │\n│ pmts_year_1139T                ┆ Year of payment for an active contract (num_group1 - existing   ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_2_4\", \"train_credit_bureau_a_2_9\", …   │\n│                                ┆ contract, num_group2 - payment).                                ┆              ┆ \"train_credit_bureau_a_2_7\"]                                   │\n│ pmts_year_507T                 ┆ Payment year for a closed credit contract (num_group1 -         ┆ [\"Float64\"]  ┆ [\"train_credit_bureau_a_2_4\", \"train_credit_bureau_a_2_9\", …   │\n│                                ┆ terminated contract, num_group2 - payment).                     ┆              ┆ \"train_credit_bureau_a_2_7\"]                                   │\n│ relatedpersons_role_762T       ┆ Relationship type of a client's related person (num_group1 -    ┆ [\"Utf8\"]     ┆ [\"train_person_2\"]                                             │\n│                                ┆ person, num_group2 - related person).                           ┆              ┆                                                                │\n│ relationshiptoclient_415T      ┆ Relationship to the client.                                     ┆ [\"Utf8\"]     ┆ [\"train_person_1\"]                                             │\n│ relationshiptoclient_642T      ┆ Relationship to the client.                                     ┆ [\"Utf8\"]     ┆ [\"train_person_1\"]                                             │\n│ riskassesment_302T             ┆ Estimated probability that the client will default on their     ┆ [\"Utf8\"]     ┆ [\"train_static_cb_0\"]                                          │\n│                                ┆ credit obligation within the next year.                         ┆              ┆                                                                │\n│ riskassesment_940T             ┆ Estimate of client's creditworthiness.                          ┆ [\"Float64\"]  ┆ [\"train_static_cb_0\"]                                          │\n└────────────────────────────────┴─────────────────────────────────────────────────────────────────┴──────────────┴────────────────────────────────────────────────────────────────┘\n","output_type":"stream"}]},{"cell_type":"code","source":"def handle_dates(df: pl.LazyFrame) -> pl.LazyFrame:\n    for col in df.columns:\n        if col.endswith('D'):\n            df = df.with_columns(pl.col(col) - pl.col('date_decision'))\n            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n            \n    df = df.with_columns([pl.col('date_decision').dt.year().alias('year').cast(pl.Int16), pl.col('date_decision').dt.month().alias('month').cast(pl.UInt8), pl.col('date_decision').dt.weekday().alias('week_num').cast(pl.UInt8)])\n        \n    return df.drop('date_decision', 'MONTH', 'WEEK_NUM');\n\n\ndef filter_cols(df: pl.LazyFrame) -> pl.LazyFrame:\n    for col in df.columns:\n        if col not in ['case_id', 'year', 'month', 'week_num', 'target']:\n            null_pct = df[col].is_null().mean()\n\n            if null_pct > 0.95:\n                df = df.drop(col)\n\n    for col in df.columns:\n        if (col not in ['case_id', 'year', 'month', 'week_num', 'target']) & (df[col].dtype == pl.String):\n            freq = df[col].n_unique()\n\n            if (freq > 200) | (freq == 1):\n                df = df.drop(col)\n\n    return df\n\n\ndef aggregate(df: pl.LazyFrame) -> pl.LazyFrame:\n    aggs: list = [];\n    cols: list[str] = df.columns;\n        \n    for col in cols:\n        if col[-1] in ('P', 'M', 'A', 'D', 'T', 'L') or 'num_group' in col:\n            for method in [pl.max, pl.min, pl.first, pl.last]:\n                aggs.append(method(col).alias(f'{method.__name__}_{col}'))\n                \n        if col.endswith(('P', 'A', 'D')):\n            aggs.append(pl.col(col).mean().alias(f'mean_{col}'))\n            \n        if col.endswith('M'):\n            aggs.append(pl.col(col).drop_nulls().mode().first().alias(f'mode_{col}'))\n    \n    return df.group_by('case_id').agg(aggs);\n\n\ndef read_file(path: str, depth: int=None) -> pl.LazyFrame:\n    df: pl.LazyFrame = pl.scan_parquet(path, low_memory=True).pipe(DTypeHandler.change_dtypes)\n    \n    if depth in (1, 2):\n        df = aggregate(df)\n    \n    return df\n\n\ndef read_files(regex_path: str, depth=None) -> pl.LazyFrame:\n    chunks: list[pl.LazyFrame] = []\n    for path in glob(str(regex_path)):\n        df: pl.LazyFrame = pl.scan_parquet(path, low_memory=True).pipe(DTypeHandler.change_dtypes)\n        \n        if depth in [1, 2]:\n            df = aggregate(df)\n        \n        chunks.append(df)\n        \n    df = pl.concat(chunks, how='vertical_relaxed')\n    df = df.unique(subset=['case_id'])\n    \n    return df\n\n\ndef join_dataframes(df_base: pl.LazyFrame, depth_0: list[pl.LazyFrame], depth_1: list[pl.LazyFrame], depth_2: list[pl.LazyFrame]) -> pl.DataFrame:\n    for (i, df) in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how='left', on='case_id', suffix=f'_{i}')\n        \n    df_base = df_base.pipe(handle_dates).collect()\n    \n    return df_base\n\n\ndef reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n    print(f'Memory usage of dataframe \\'{name}\\' is {round(df.estimated_size(\"mb\"), 2)} MB.')\n    \n    int_types = [pl.Int8, pl.Int16, pl.Int32, pl.Int64]\n    float_types = [pl.Float32, pl.Float64]\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        c_min = df[col].min()\n        c_max = df[col].max()\n        \n        if col_type in int_types:\n            if c_min is not None and c_max is not None:\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df = df.with_columns(df[col].cast(pl.Int8))\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df = df.with_columns(df[col].cast(pl.Int16))\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df = df.with_columns(df[col].cast(pl.Int32))\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df = df.with_columns(df[col].cast(pl.Int64))\n        elif col_type in float_types:\n            if c_min is not None and c_max is not None:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df = df.with_columns(df[col].cast(pl.Float32))\n        else:\n            pass\n\n    print(f'Memory usage of dataframe \\'{name}\\' became {round(df.estimated_size(\"mb\"), 2)} MB.')\n    \n    return df\n\n\ndef to_pandas(df: pl.DataFrame, cat_cols:list[str]=None) -> (pd.DataFrame, list[str]):\n    df = df.to_pandas()\n    \n    if cat_cols is None:\n        cat_cols = list(df.select_dtypes('object').columns)\n    \n    df[cat_cols] = df[cat_cols].astype('category')\n    \n    return df, cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:10:18.457390Z","iopub.execute_input":"2024-05-09T06:10:18.457818Z","iopub.status.idle":"2024-05-09T06:10:18.492247Z","shell.execute_reply.started":"2024-05-09T06:10:18.457788Z","shell.execute_reply":"2024-05-09T06:10:18.490866Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_store: dict = {\n    'df_base': read_file(TRAIN_DIR / 'train_base.parquet'),\n    'depth_0': [\n        read_file(TRAIN_DIR / 'train_static_cb_0.parquet'),\n        read_files(TRAIN_DIR / 'train_static_0_*.parquet'),\n    ],\n    'depth_1': [\n        read_files(TRAIN_DIR / 'train_applprev_1_*.parquet', 1),\n        read_file(TRAIN_DIR / 'train_tax_registry_a_1.parquet', 1),\n        read_file(TRAIN_DIR / 'train_tax_registry_b_1.parquet', 1),\n        read_file(TRAIN_DIR / 'train_tax_registry_c_1.parquet', 1),\n#         read_files(TRAIN_DIR / 'train_credit_bureau_a_1_*.parquet', 1),\n        read_file(TRAIN_DIR / 'train_credit_bureau_b_1.parquet', 1),\n        read_file(TRAIN_DIR / 'train_other_1.parquet', 1),\n        read_file(TRAIN_DIR / 'train_person_1.parquet', 1),\n        read_file(TRAIN_DIR / 'train_deposit_1.parquet', 1),\n        read_file(TRAIN_DIR / 'train_debitcard_1.parquet', 1),\n    ],\n    'depth_2': [\n#         read_files(TRAIN_DIR / 'train_credit_bureau_a_2_*.parquet', 2),\n        read_file(TRAIN_DIR / 'train_credit_bureau_b_2.parquet', 2),\n    ]\n}\n\ndf_train: pl.DataFrame = join_dataframes(**data_store)\ndf_train = reduce_memory_usage(df_train, 'df_train')\n\ndel data_store\ngc.collect()\n\nprint(f'Train data shape:\\t{df_train.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:10:18.493643Z","iopub.execute_input":"2024-05-09T06:10:18.494050Z","iopub.status.idle":"2024-05-09T06:12:05.630323Z","shell.execute_reply.started":"2024-05-09T06:10:18.494014Z","shell.execute_reply":"2024-05-09T06:12:05.629233Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Memory usage of dataframe 'df_train' is 8169.01 MB.\nMemory usage of dataframe 'df_train' became 5006.72 MB\nTrain data shape:\t(1526659, 927)\n","output_type":"stream"}]},{"cell_type":"code","source":"data_store: dict = {\n    'df_base': read_file(TEST_DIR / 'test_base.parquet'),\n    'depth_0': [\n        read_file(TEST_DIR / 'test_static_cb_0.parquet'),\n        read_files(TEST_DIR / 'test_static_0_*.parquet'),\n    ],\n    'depth_1': [\n        read_files(TEST_DIR / 'test_applprev_1_*.parquet', 1),\n        read_file(TEST_DIR / 'test_tax_registry_a_1.parquet', 1),\n        read_file(TEST_DIR / 'test_tax_registry_b_1.parquet', 1),\n        read_file(TEST_DIR / 'test_tax_registry_c_1.parquet', 1),\n#         read_files(TEST_DIR / 'test_credit_bureau_a_1_*.parquet', 1),\n        read_file(TEST_DIR / 'test_credit_bureau_b_1.parquet', 1),\n        read_file(TEST_DIR / 'test_other_1.parquet', 1),\n        read_file(TEST_DIR / 'test_person_1.parquet', 1),\n        read_file(TEST_DIR / 'test_deposit_1.parquet', 1),\n        read_file(TEST_DIR / 'test_debitcard_1.parquet', 1),\n    ],\n    'depth_2': [\n#         read_files(TEST_DIR / 'test_credit_bureau_a_2_*.parquet', 2),\n        read_file(TEST_DIR / 'test_credit_bureau_b_2.parquet', 2),\n    ]\n}\n\ndf_test: pl.DataFrame = join_dataframes(**data_store)\n\ndel data_store\ngc.collect()\n\nprint(f'Test data shape:\\t{df_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:12:05.631560Z","iopub.execute_input":"2024-05-09T06:12:05.631948Z","iopub.status.idle":"2024-05-09T06:12:06.035158Z","shell.execute_reply.started":"2024-05-09T06:12:05.631919Z","shell.execute_reply":"2024-05-09T06:12:06.034206Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Test data shape:\t(10, 926)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = df_train.pipe(filter_cols)\ndf_test = df_test.select([col for col in df_train.columns if col != 'target'])\n\nprint('train data shape:\\t', df_train.shape)\nprint('test data shape:\\t', df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:12:06.036285Z","iopub.execute_input":"2024-05-09T06:12:06.036613Z","iopub.status.idle":"2024-05-09T06:12:12.451784Z","shell.execute_reply.started":"2024-05-09T06:12:06.036586Z","shell.execute_reply":"2024-05-09T06:12:12.450715Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"train data shape:\t (1526659, 517)\ntest data shape:\t (10, 516)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train, cat_cols = to_pandas(df_train)\ndf_test, cat_cols = to_pandas(df_test, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:12:12.453034Z","iopub.execute_input":"2024-05-09T06:12:12.453369Z","iopub.status.idle":"2024-05-09T06:12:54.446185Z","shell.execute_reply.started":"2024-05-09T06:12:12.453340Z","shell.execute_reply":"2024-05-09T06:12:54.445339Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class VotingModel(BaseEstimator, ClassifierMixin):\n    def __init__(self, estimators: list):\n        super().__init__()\n        self.estimators = estimators\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:12:54.447459Z","iopub.execute_input":"2024-05-09T06:12:54.447769Z","iopub.status.idle":"2024-05-09T06:12:54.455090Z","shell.execute_reply.started":"2024-05-09T06:12:54.447743Z","shell.execute_reply":"2024-05-09T06:12:54.454069Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X = df_train.drop(columns=['target', 'case_id', 'week_num'])\ny = df_train['target']\n\nweeks = df_train['week_num']\n\ncv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'max_depth': 8,\n    'learning_rate': 0.05,\n    'n_estimators': 1000,\n    'colsample_bytree': 0.8, \n    'colsample_bynode': 0.8,\n    'verbose': -1,\n    'random_state': 42,\n    'device': 'gpu',\n}\n\nfitted_models = []\n\nfor idx_train, idx_valid in cv.split(X, y, groups=weeks):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n    model = lgb.LGBMClassifier(**params)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid, y_valid)],\n        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)]\n    )\n\n    fitted_models.append(model)\n\nmodel = VotingModel(fitted_models)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:12:54.458383Z","iopub.execute_input":"2024-05-09T06:12:54.458687Z","iopub.status.idle":"2024-05-09T06:34:57.936399Z","shell.execute_reply.started":"2024-05-09T06:12:54.458662Z","shell.execute_reply":"2024-05-09T06:34:57.935274Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.821995\n[200]\tvalid_0's auc: 0.831746\n[300]\tvalid_0's auc: 0.835394\n[400]\tvalid_0's auc: 0.836584\n[500]\tvalid_0's auc: 0.837041\n[600]\tvalid_0's auc: 0.83745\n[700]\tvalid_0's auc: 0.83777\n[800]\tvalid_0's auc: 0.837812\n[900]\tvalid_0's auc: 0.838043\n[1000]\tvalid_0's auc: 0.838167\nDid not meet early stopping. Best iteration is:\n[940]\tvalid_0's auc: 0.838174\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.832973\n[200]\tvalid_0's auc: 0.841958\n[300]\tvalid_0's auc: 0.845271\n[400]\tvalid_0's auc: 0.846458\n[500]\tvalid_0's auc: 0.84713\n[600]\tvalid_0's auc: 0.847432\n[700]\tvalid_0's auc: 0.847785\n[800]\tvalid_0's auc: 0.847981\n[900]\tvalid_0's auc: 0.848129\n[1000]\tvalid_0's auc: 0.848168\nDid not meet early stopping. Best iteration is:\n[999]\tvalid_0's auc: 0.848178\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.815651\n[200]\tvalid_0's auc: 0.824562\n[300]\tvalid_0's auc: 0.827426\n[400]\tvalid_0's auc: 0.828576\n[500]\tvalid_0's auc: 0.829069\n[600]\tvalid_0's auc: 0.829442\n[700]\tvalid_0's auc: 0.829609\n[800]\tvalid_0's auc: 0.829886\n[900]\tvalid_0's auc: 0.829939\nEarly stopping, best iteration is:\n[863]\tvalid_0's auc: 0.830016\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.822207\n[200]\tvalid_0's auc: 0.830266\n[300]\tvalid_0's auc: 0.833421\n[400]\tvalid_0's auc: 0.834449\n[500]\tvalid_0's auc: 0.834831\n[600]\tvalid_0's auc: 0.835368\n[700]\tvalid_0's auc: 0.83558\n[800]\tvalid_0's auc: 0.835836\n[900]\tvalid_0's auc: 0.835995\n[1000]\tvalid_0's auc: 0.836096\nDid not meet early stopping. Best iteration is:\n[972]\tvalid_0's auc: 0.836129\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's auc: 0.812335\n[200]\tvalid_0's auc: 0.820716\n[300]\tvalid_0's auc: 0.823616\n[400]\tvalid_0's auc: 0.824531\n[500]\tvalid_0's auc: 0.824904\n[600]\tvalid_0's auc: 0.825221\n[700]\tvalid_0's auc: 0.825437\n[800]\tvalid_0's auc: 0.825416\nEarly stopping, best iteration is:\n[735]\tvalid_0's auc: 0.825502\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test: pd.DataFrame = df_test.drop(columns=['week_num']).set_index('case_id')\n\ny_pred: pd.Series = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)\n\ndf_subm = pd.read_csv(ROOT / 'sample_submission.csv')\ndf_subm = df_subm.set_index('case_id')\n\ndf_subm['score'] = y_pred\n\nwith pl.Config(fmt_str_lengths=1000, tbl_rows=-1, tbl_width_chars=180):\n    print(df_subm)\n    \ndf_subm.to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:34:57.937913Z","iopub.execute_input":"2024-05-09T06:34:57.938240Z","iopub.status.idle":"2024-05-09T06:34:58.664646Z","shell.execute_reply.started":"2024-05-09T06:34:57.938214Z","shell.execute_reply":"2024-05-09T06:34:58.663477Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"            score\ncase_id          \n57543    0.011007\n57549    0.041158\n57551    0.006355\n57552    0.012732\n57569    0.094646\n57630    0.011707\n57631    0.050166\n57632    0.025001\n57633    0.063634\n57634    0.025919\n","output_type":"stream"}]}]}